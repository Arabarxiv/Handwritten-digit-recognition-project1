{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digit Recognition (PART1) üöÄ\n",
        "\n",
        "In this notebook, we're embarking on an exciting journey to develop a basic understanding of building and training a neural network for recognizing handwritten digits using PyTorch. We'll dive into the world of the MNIST dataset üìä, a classic in the machine learning community, renowned for its collection of black and white images of digits from 0 to 9.\n",
        "\n",
        "This lab is designed to empower you with key skills and concepts in deep learning:\n",
        "\n",
        "    üß† Understand Gradient Descent from Scratch\n",
        "    üìö Load Dataset using PyTorch\n",
        "    üèóÔ∏è Build a Neural Network using PyTorch\n",
        "    üöÄ Start the training script and watch your neural network learn!\n",
        "\n",
        "Let's get started and unlock the potential of neural networks in digit recognition! üí°üîç\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "91Z7g4qzxYtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART1: Gradient Descent\n",
        "GD is an algorithm that help us find the minimum of the error function. It is an important algorithm used in all deep learning projects.\n",
        "\n",
        "In this first part, we are going to develop GD from scratch to learn how it works. We are going to go through GD using our example in the course (pass/fail)."
      ],
      "metadata": {
        "id": "1Owj1F2eyKWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8IUFO_KGyl6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mmc1EHUxIaI"
      },
      "outputs": [],
      "source": [
        "# DO NOT TOUCH (These are helper function)\n",
        "# Some helper functions for plotting and drawing lines\n",
        "def plot_points(X, y):\n",
        "    passed = X[np.argwhere(y==1)]\n",
        "    failed = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in failed], [s[0][1] for s in failed], s = 25, color = 'blue', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in passed], [s[0][1] for s in passed], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "# Generating random dataset.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "mean_pass = [0.5, 0.8]\n",
        "cov_pass = [[0.01, 0], [0, 0.02]]  # Diagonal covariance\n",
        "\n",
        "mean_fail = [0.3, 0.3]\n",
        "cov_fail = [[0.01, 0], [0, 0.02]]\n",
        "\n",
        "# Generate 'pass' and 'fail' data\n",
        "pass_students = np.random.multivariate_normal(mean_pass, cov_pass, 50)\n",
        "fail_students = np.random.multivariate_normal(mean_fail, cov_fail, 50)\n",
        "\n",
        "# Create dataframes for pass and fail students\n",
        "df_pass = pd.DataFrame(pass_students, columns=['Hours', 'Attendance'])\n",
        "df_pass['Label'] = 1\n",
        "\n",
        "df_fail = pd.DataFrame(fail_students, columns=['Hours', 'Attendance'])\n",
        "df_fail['Label'] = 0\n",
        "\n",
        "# Combine the two dataframes into one\n",
        "df = pd.concat([df_pass, df_fail], ignore_index=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Y_s3DoeHyxnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df[[\"Hours\",\"Attendance\"]])\n",
        "y = np.array(df['Label'])\n",
        "plot_points(X,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxrf8nIOynWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Implementing the basic functions\n",
        "Here is your turn to shine. Implement the following formulas, as explained in the text.\n",
        "- Sigmoid activation function\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "- Output (prediction) formula\n",
        "\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
        "\n",
        "- Error function\n",
        "\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
        "\n",
        "- The function that updates the weights\n",
        "\n",
        "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
        "\n",
        "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
      ],
      "metadata": {
        "id": "cvP8hDzH0Tlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the following functions\n",
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return\n",
        "\n",
        "# Output (prediction) formula\n",
        "def output_formula(features, weights, bias):\n",
        "    return\n",
        "\n",
        "# Error (log-loss) formula\n",
        "def error_formula(y, output):\n",
        "    return\n",
        "\n",
        "\n",
        "# Gradient descent step\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    output = output_formula(x, weights, bias)\n",
        "    error = -\n",
        "    weights -=\n",
        "    bias -=\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "0hfjXx6oz_oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Function (using GD)"
      ],
      "metadata": {
        "id": "rE4lPtcD0mZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "np.random.seed(44)\n",
        "epochs =\n",
        "learnrate =\n",
        "\n",
        "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
        "    errors = []\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "    bias = 0\n",
        "\n",
        "    # CHANGE HERE\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features, targets):\n",
        "            output =\n",
        "            error =\n",
        "            weights, bias =\n",
        "\n",
        "        # STOP CHANGING\n",
        "        # Printing out the log-loss error on the training set\n",
        "        out = output_formula(features, weights, bias)\n",
        "        loss = np.mean(error_formula(targets, out))\n",
        "        errors.append(loss)\n",
        "        if e % (epochs / 10) == 0:\n",
        "            print(\"\\n========== Epoch\", e,\"==========\")\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            predictions = out > 0.5\n",
        "            accuracy = np.mean(predictions == targets)\n",
        "            print(\"Accuracy: \", accuracy)\n",
        "        if graph_lines and e % (epochs / 100) == 0:\n",
        "            display(-weights[0]/weights[1], -bias/weights[1])\n",
        "\n",
        "\n",
        "    # Plotting the solution boundary\n",
        "    plt.title(\"Solution boundary\")\n",
        "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_points(features, targets)\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the error\n",
        "    plt.title(\"Error Plot\")\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Error')\n",
        "    plt.plot(errors)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7DnhNpTc0ZzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üèãÔ∏è‚Äç‚ôÇÔ∏è Training Time: Watch the Algorithm Learn!\n",
        "\n",
        "By initiating the training function, we're going to witness our model's learning process in real-time! Here's what to expect as the magic unfolds:\n",
        "\n",
        "* **Progress Updates:** Get ready for a series of 10 informative updates, complete with the current training loss and accuracy metrics. Watch as the numbers evolve, showcasing the algorithm's improvement with each epoch.\n",
        "\n",
        "* **Visual Insights:** A dynamic plot will bring our data to life, along with the progression of boundary lines that our model generates. Keep an eye on the black line ‚Äì that's our model's final hypothesis. Observe how each iteration nudges the boundaries closer to the ideal fit. It's like watching the algorithm's thought process visualized!\n",
        "\n",
        "* **Error Trends:** Another plot will graph the journey of our error function. Anticipate a satisfying downward trend as the epochs roll by, indicating that our model is minimizing mistakes and maximizing precision.\n",
        "\n",
        "So, let's hit 'Run' and watch the algorithm flex its computational muscles towards achieving peak performance! üöÄ"
      ],
      "metadata": {
        "id": "wjSOTu4r1K92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(X, y, epochs, learnrate, True)"
      ],
      "metadata": {
        "id": "RGPhVQpu05Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: experiment with different learning rates !"
      ],
      "metadata": {
        "id": "fs47mZNG1g54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART2: PyTorch üõ†Ô∏è\n",
        "\n",
        "PyTorch is an open-source deep learning library, widely used for applications such as computer vision and natural language processing. It is known for its flexibility and ease of use, especially when it comes to building and experimenting with neural network architectures. Developed by the Facebook AI Research lab, PyTorch provides two high-level features: Tensor computation (like NumPy) with strong GPU acceleration, and Deep Neural Networks built on a tape-based autograd system. It's particularly favored for its dynamic computational graph and efficient memory usage, which makes it a go-to tool for researchers and developers alike.\n",
        "\n",
        "### Loading the MNIST Dataset\n",
        "The MNIST dataset is a collection of 70,000 grayscale images of handwritten digits. We will load this dataset and prepare it for training our neural network. It is a \"Hello, World!\" of computer vision, providing a manageable dataset to train basic image processing and pattern recognition algorithms.\n",
        "\n",
        "Let's dive into the data loading process!\n",
        "\n"
      ],
      "metadata": {
        "id": "WTYCa_3t1vBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transformations applied on each image\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Loading MNIST dataset from torchvision\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "# TODO: Load the test set (Use the same thing as train_set)\n",
        "test_set =\n",
        "test_loader ="
      ],
      "metadata": {
        "id": "sUfzto4_2Lqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying Images and Their Labels\n",
        "Let's visualize some of the images in our dataset along with their corresponding labels.\n"
      ],
      "metadata": {
        "id": "D7i9YtTv2br6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to display images\n",
        "def show_images(images, labels):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i in range(20):\n",
        "        plt.subplot(2, 10, i+1)\n",
        "        plt.imshow(images[i].numpy().squeeze(), cmap='gray_r')\n",
        "        plt.title(labels[i].item())\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Display images\n",
        "show_images(images, labels)\n",
        "\n",
        "# TODO: Explore different batches and display their images and labels"
      ],
      "metadata": {
        "id": "mFUCgFDD2Z3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Fully Connected Neural Network\n",
        "We will now define our neural network architecture. We'll start with a simple fully connected network with a single hidden layer.\n",
        "\n",
        "When using an image in a fully connected neural network, we need to change its dimension.\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*IWUxuBpqn2VuV-7Ubr01ng.png)"
      ],
      "metadata": {
        "id": "AuRCg-j42ftg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "# Fill the size of the input and output to the layer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Neural network with a single fully connected layer\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(, )  # TODO: Define the fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, )  # Flatten the image\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Create an instance of the network\n",
        "model = SimpleNet()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ubtYih7a2i6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "Next, we'll define the training loop. We will train our model using gradient descent.\n"
      ],
      "metadata": {
        "id": "dHxycsC23AIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete the training loop\n",
        "        pass\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "id": "2mw-uqnS2-pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "Finally, let's evaluate the performance of our model on the test dataset.\n"
      ],
      "metadata": {
        "id": "3mgc-dyt3K40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Write the evaluation loop"
      ],
      "metadata": {
        "id": "p4EPFOqy3LNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "In this notebook, we covered the basics of loading a dataset, visualizing it, building a simple neural network, and training it. We encourage you to experiment with different network architectures, learning rates, and other hyperparameters.\n",
        "\n",
        "## Additional Resources\n",
        "- [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- [Neural Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning\n"
      ],
      "metadata": {
        "id": "KyHEj3T53NQ7"
      }
    }
  ]
}